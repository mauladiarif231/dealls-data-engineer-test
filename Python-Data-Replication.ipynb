{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5670,"status":"ok","timestamp":1728401169818,"user":{"displayName":"mauladi arif Iqbal","userId":"11095266404629954107"},"user_tz":-420},"id":"bosEzd0XsoTF","outputId":"49ff9fa1-dafa-4559-fdac-a7dc0d259532"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (16.1.0)\n","Requirement already satisfied: memory-profiler in /usr/local/lib/python3.10/dist-packages (0.61.0)\n","Collecting ijson\n","  Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler) (5.9.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Downloading ijson-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: ijson\n","Successfully installed ijson-3.3.0\n"]}],"source":["!pip install pandas pyarrow memory-profiler ijson"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0GfXa0iu_BJ","executionInfo":{"status":"ok","timestamp":1728402232716,"user_tz":-420,"elapsed":234499,"user":{"displayName":"mauladi arif Iqbal","userId":"11095266404629954107"}},"outputId":"2a9eb46b-f336-414f-c134-c02678c141df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting processing of /content/drive/MyDrive/source/yelp_academic_dataset_review.json\n","Processed 100000 rows...\n","Processed 200000 rows...\n","Processed 300000 rows...\n","Processed 400000 rows...\n","Processed 500000 rows...\n","Processed 600000 rows...\n","Processed 700000 rows...\n","Processed 800000 rows...\n","Processed 900000 rows...\n","Processed 1000000 rows...\n","Processed 1100000 rows...\n","Processed 1200000 rows...\n","Processed 1300000 rows...\n","Processed 1400000 rows...\n","Processed 1500000 rows...\n","Processed 1600000 rows...\n","Processed 1700000 rows...\n","Processed 1800000 rows...\n","Processed 1900000 rows...\n","Processed 2000000 rows...\n","Processed 2100000 rows...\n","Processed 2200000 rows...\n","Processed 2300000 rows...\n","Processed 2400000 rows...\n","Processed 2500000 rows...\n","Processed 2600000 rows...\n","Processed 2700000 rows...\n","Processed 2800000 rows...\n","Processed 2900000 rows...\n","Processed 3000000 rows...\n","Processed 3100000 rows...\n","Processed 3200000 rows...\n","Processed 3300000 rows...\n","Processed 3400000 rows...\n","Processed 3500000 rows...\n","Processed 3600000 rows...\n","Processed 3700000 rows...\n","Processed 3800000 rows...\n","Processed 3900000 rows...\n","Processed 4000000 rows...\n","Processed 4100000 rows...\n","Processed 4200000 rows...\n","Processed 4300000 rows...\n","Processed 4400000 rows...\n","Processed 4500000 rows...\n","Processed 4600000 rows...\n","Processed 4700000 rows...\n","Processed 4800000 rows...\n","Processed 4900000 rows...\n","Processed 5000000 rows...\n","Processed 5100000 rows...\n","Processed 5200000 rows...\n","Processed 5300000 rows...\n","Processed 5400000 rows...\n","Processed 5500000 rows...\n","Processed 5600000 rows...\n","Processed 5700000 rows...\n","Processed 5800000 rows...\n","Processed 5900000 rows...\n","Processed 6000000 rows...\n","Processed 6100000 rows...\n","Processed 6200000 rows...\n","Processed 6300000 rows...\n","Processed 6400000 rows...\n","Processed 6500000 rows...\n","Processed 6600000 rows...\n","Processed 6700000 rows...\n","Processed 6800000 rows...\n","Processed 6900000 rows...\n","\n","Performance Metrics:\n","Runtime: 233.77 seconds\n","Memory Usage: 30.87 MB\n","Peak Memory: 205.99 MB\n","\n","Parquet File Details:\n","File Size: 3086293173 bytes\n","Schema:\n","<pyarrow._parquet.ParquetSchema object at 0x7b222340f980>\n","required group field_id=-1 schema {\n","  optional binary field_id=-1 review_id (String);\n","  optional binary field_id=-1 user_id (String);\n","  optional binary field_id=-1 business_id (String);\n","  optional double field_id=-1 stars;\n","  optional int64 field_id=-1 useful;\n","  optional int64 field_id=-1 funny;\n","  optional int64 field_id=-1 cool;\n","  optional binary field_id=-1 text (String);\n","  optional binary field_id=-1 date (String);\n","}\n","\n","\n","Data Comparison:\n","Total Rows: 6990280\n","stars_sum: 26203650.0\n","useful_sum: 8280748\n","funny_sum: 2282743\n","cool_sum: 3485476\n"]}],"source":["import json\n","import pandas as pd\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","from typing import Generator, Dict, Any\n","import time\n","import psutil\n","import os\n","\n","def get_memory_usage() -> float:\n","    \"\"\"Return current memory usage in MB\"\"\"\n","    return psutil.Process().memory_info().rss / (1024 * 1024)\n","\n","def stream_jsonl(file_path: str) -> Generator[Dict[Any, Any], None, None]:\n","    \"\"\"Stream JSON objects from a large JSON Lines file\"\"\"\n","    with open(file_path, 'r') as file:\n","        for line in file:\n","            try:\n","                yield json.loads(line.strip())\n","            except json.JSONDecodeError:\n","                print(f\"Skipping invalid JSON line: {line[:50]}...\")\n","                continue\n","\n","def flatten_dict(d: Dict[Any, Any], parent_key: str = '', sep: str = '_') -> Dict[str, Any]:\n","    \"\"\"Flatten nested dictionaries\"\"\"\n","    items = []\n","    for k, v in d.items():\n","        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n","        if isinstance(v, dict):\n","            items.extend(flatten_dict(v, new_key, sep=sep).items())\n","        else:\n","            items.append((new_key, v))\n","    return dict(items)\n","\n","def process_jsonl_to_parquet(input_jsonl_path: str, output_parquet_path: str, chunk_size: int = 10000):\n","    \"\"\"\n","    Process large JSON Lines file to Parquet format using chunking\n","\n","    Args:\n","        input_jsonl_path: Path to input JSON Lines file\n","        output_parquet_path: Path to output Parquet file\n","        chunk_size: Number of records to process at once\n","\n","    Returns:\n","        dict: Performance metrics and data comparison\n","    \"\"\"\n","    start_time = time.time()\n","    initial_memory = get_memory_usage()\n","\n","    # Initialize metrics\n","    total_rows = 0\n","    metrics = {\n","        'stars_sum': 0,\n","        'useful_sum': 0,\n","        'funny_sum': 0,\n","        'cool_sum': 0\n","    }\n","\n","    # Initialize schema\n","    schema = None\n","    writer = None\n","\n","    chunk = []\n","    try:\n","        for item in stream_jsonl(input_jsonl_path):\n","            flattened_item = flatten_dict(item)\n","            chunk.append(flattened_item)\n","\n","            # Update metrics\n","            total_rows += 1\n","            metrics['stars_sum'] += flattened_item.get('stars', 0)\n","            metrics['useful_sum'] += flattened_item.get('useful', 0)\n","            metrics['funny_sum'] += flattened_item.get('funny', 0)\n","            metrics['cool_sum'] += flattened_item.get('cool', 0)\n","\n","            if len(chunk) >= chunk_size:\n","                df = pd.DataFrame(chunk)\n","\n","                if schema is None:\n","                    schema = pa.Schema.from_pandas(df)\n","                    writer = pq.ParquetWriter(output_parquet_path, schema)\n","\n","                table = pa.Table.from_pandas(df, schema=schema)\n","                writer.write_table(table)\n","                chunk = []\n","\n","                # Print progress\n","                if total_rows % (chunk_size * 10) == 0:\n","                    print(f\"Processed {total_rows} rows...\")\n","\n","        # Write remaining records\n","        if chunk:\n","            df = pd.DataFrame(chunk)\n","            if schema is None:\n","                schema = pa.Schema.from_pandas(df)\n","                writer = pq.ParquetWriter(output_parquet_path, schema)\n","            table = pa.Table.from_pandas(df, schema=schema)\n","            writer.write_table(table)\n","\n","    finally:\n","        if writer:\n","            writer.close()\n","\n","    end_time = time.time()\n","    final_memory = get_memory_usage()\n","\n","    # Get Parquet file details\n","    parquet_file = pq.ParquetFile(output_parquet_path)\n","    parquet_schema = parquet_file.schema\n","    parquet_size = os.path.getsize(output_parquet_path)\n","\n","    return {\n","        'performance': {\n","            'runtime_seconds': end_time - start_time,\n","            'memory_usage_mb': final_memory - initial_memory,\n","            'peak_memory_mb': psutil.Process().memory_info().rss / (1024 * 1024)\n","        },\n","        'parquet_details': {\n","            'file_size_bytes': parquet_size,\n","            'schema': str(parquet_schema)\n","        },\n","        'data_comparison': {\n","            'total_rows': total_rows,\n","            'metrics': metrics\n","        }\n","    }\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    INPUT_JSONL_PATH = '/content/drive/MyDrive/source/yelp_academic_dataset_review.json'\n","    OUTPUT_PARQUET_PATH = '/content/drive/MyDrive/output/reviews.parquet'\n","\n","    print(f\"Starting processing of {INPUT_JSONL_PATH}\")\n","\n","    # Process the data and get metrics\n","    metrics = process_jsonl_to_parquet(INPUT_JSONL_PATH, OUTPUT_PARQUET_PATH)\n","\n","    # Print metrics\n","    print(\"\\nPerformance Metrics:\")\n","    print(f\"Runtime: {metrics['performance']['runtime_seconds']:.2f} seconds\")\n","    print(f\"Memory Usage: {metrics['performance']['memory_usage_mb']:.2f} MB\")\n","    print(f\"Peak Memory: {metrics['performance']['peak_memory_mb']:.2f} MB\")\n","\n","    print(\"\\nParquet File Details:\")\n","    print(f\"File Size: {metrics['parquet_details']['file_size_bytes']} bytes\")\n","    print(f\"Schema:\\n{metrics['parquet_details']['schema']}\")\n","\n","    print(\"\\nData Comparison:\")\n","    print(f\"Total Rows: {metrics['data_comparison']['total_rows']}\")\n","    for metric, value in metrics['data_comparison']['metrics'].items():\n","        print(f\"{metric}: {value}\")"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1JXhRwISTYj7jFDnNK-waUR_LVRQ8I04P","authorship_tag":"ABX9TyOhg+JkImTSJcIP+R3Hjcnv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}